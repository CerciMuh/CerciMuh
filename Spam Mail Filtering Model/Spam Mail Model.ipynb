{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EAdECfOXX_Gv"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "**Note:** Use Jupyter Notebook or Google Colab (NOT Jupyter Lab).\n",
    "\n",
    "The messages within the datasets have already been classified as spam or ham (not spam).\n",
    "\n",
    "The implementation of a Na√Øve Bayes Spam Filter is relatively straightforward without scikit-learn, and this exercise will help you understand the implementation details.\n",
    "\n",
    "Bayes Theorem can give us the probability that a message is spam \\(S\\) for a given event \\(E\\):\n",
    "\n",
    "$$\n",
    "P\\left(S \\mid E\\right) = \\frac{P\\left(E \\mid S\\right)P\\left(S\\right)}{P\\left(E \\mid S\\right)P\\left(S\\right) + P\\left(E \\mid \\lnot S\\right)P\\left(\\lnot S\\right)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\(P\\left(S \\mid E\\right)\\) is the probability that the message is spam given the event occurred.\n",
    "- \\(P\\left(S\\right)\\) is the prior probability that a message is spam.\n",
    "- \\(P\\left(\\lnot S\\right)\\) is the prior probability that a message is not spam.\n",
    "\n",
    "Note: \\(P\\left(S\\right)\\) and \\(P\\left(\\lnot S\\right)\\) are prior values or beliefs. You can calculate them using the dataset's spam and ham counts, or assume arbitrary values, like 80% of emails are spam and 20% are not. The filter's success depends on these prior values.\n",
    "\n",
    "- \\(P\\left(E \\mid S\\right)\\) is the probability that event \\(E\\) occurs in spam emails.\n",
    "- \\(P\\left(E \\mid \\lnot S\\right)\\) is the probability that event \\(E\\) occurs in non-spam emails.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ep6k47M5X_Gw"
   },
   "source": [
    "<h3>1.  Read the dataset into a dataframe and explore</h3>\n",
    "<p>Start by importing pandas and read the dataset into a DataFrame named df.  Output the first 20 rows of the dataframe to get a general feel of how the data is structured.</p>\n",
    "<p>You may encounter the error: UnicodeDecodeError: 'utf-8' codec can't decode bytes in position 135-136: invalid continuation byte.  You don't need to edit the datafile,  as you should be able to successfully read in the datafile by changing the encoding to latin-1.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vq43eDWcX_Gx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the dataset into a DataFrame\n",
    "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "\n",
    "# Output the first 20 rows of the DataFrame\n",
    "print(df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W1HwlygYX_G2"
   },
   "source": [
    "<h3>2. Clean the data</h3>\n",
    "<p>We are only interested in words, clean the data so that all punctuations are removed.  You should be left with a dataset that only contains alpha characters (including spaces).  You should also ensure all the words are lowercase.  Store the cleaned data into a DataFrame named clean.</p>\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Category</th>\n",
    "      <th>Message</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>ham</td>\n",
    "      <td>go until jurong point crazy available only in ...</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>ham</td>\n",
    "      <td>ok lar joking wif u oni</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>spam</td>\n",
    "      <td>free entry in  a wkly comp to win fa cup final...</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>ham</td>\n",
    "      <td>u dun say so early hor u c already then say</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>ham</td>\n",
    "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JqppPCO_X_G3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Assuming 'df' contains the DataFrame with the original data\n",
    "# Create a copy of the DataFrame to work with\n",
    "clean = df.copy()\n",
    "\n",
    "# Remove punctuations and convert to lowercase\n",
    "clean['v2'] = clean['v2'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x).lower())\n",
    "\n",
    "# Output the first 20 rows of the cleaned DataFrame\n",
    "print(clean.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CLTcpAMvX_G8"
   },
   "source": [
    "<h3>3. Split the Data</h3>\n",
    "<p>Split the data into three random samples, one for training the model, one for validation and the other for testing the model.  Create DataFrames named train_data, validation_data and test_data.  The train_data DataFrame should contain 60-70% of the data, validation_data 15-20% and the test_data DataFrame the remaining data.<p>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fgKwaz1yX_G9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'clean' is your DataFrame containing the cleaned data\n",
    "# Shuffle the DataFrame\n",
    "clean = clean.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Determine the proportion of each dataset\n",
    "train_percent = 0.7  # 70% of data for training\n",
    "validation_percent = 0.15  # 15% of data for validation\n",
    "\n",
    "# Calculate split indices\n",
    "train_end = int(train_percent * len(clean))\n",
    "validation_end = train_end + int(validation_percent * len(clean))\n",
    "\n",
    "# Split the DataFrame\n",
    "train_data = clean[:train_end]\n",
    "validation_data = clean[train_end:validation_end]\n",
    "test_data = clean[validation_end:]\n",
    "\n",
    "# Output the number of entries in each set\n",
    "print(f\"Training Set: {len(train_data)} entries, {100 * len(train_data) / len(clean):.2f}% of total\")\n",
    "print(f\"Validation Set: {len(validation_data)} entries, {100 * len(validation_data) / len(clean):.2f}% of total\")\n",
    "print(f\"Test Set: {len(test_data)} entries, {100 * len(test_data) / len(clean):.2f}% of total\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EU8azvVgX_HB"
   },
   "source": [
    "<h3>4. Create a Word Frequency DataFrame</h3>\n",
    "<p>Create a new DataFrame named word_freq that contains each word with the number of times it appears in a spam and a ham message.  You should use the train_data.</p>\n",
    "<p>Below is an example of what the DataFrame would look like, <i>note</i> that your values may differ depending on how the data was split.</p>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Word</th>\n",
    "      <th>#Spam</th>\n",
    "      <th>#Ham</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>0</td>\n",
    "      <td>go</td>\n",
    "      <td>27</td>\n",
    "      <td>196</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>1</td>\n",
    "      <td>until</td>\n",
    "      <td>4</td>\n",
    "      <td>17</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>2</td>\n",
    "      <td>jurong</td>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>3</td>\n",
    "      <td>point</td>\n",
    "      <td>1</td>\n",
    "      <td>9</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>4</td>\n",
    "      <td>crazy</td>\n",
    "      <td>4</td>\n",
    "      <td>8</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>7253</td>\n",
    "      <td>salesman</td>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>7254</td>\n",
    "      <td>pity</td>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>7255</td>\n",
    "      <td>soany</td>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>7256</td>\n",
    "      <td>suggestions</td>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>7257</td>\n",
    "      <td>bitching</td>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cJxL5N2BX_HC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "word_freq = {}\n",
    "\n",
    "for index, row in train_data.iterrows():\n",
    "    words = row['v2'].split()\n",
    "    label = row['v1']\n",
    "    for word in words:\n",
    "        if word not in word_freq:\n",
    "            word_freq[word] = {'spam_count': 0, 'ham_count': 0}\n",
    "        word_freq[word][f'{label}_count'] += 1\n",
    "\n",
    "# Create a DataFrame from the word frequency dictionary\n",
    "word_freq_df = pd.DataFrame(word_freq).transpose().fillna(0)\n",
    "\n",
    "# Output the first few rows of the word_freq DataFrame\n",
    "print(word_freq_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1DBTOQpJbSqE"
   },
   "source": [
    "<h3>5. Visualise the Data</h3>\n",
    "<p>Let's use a Word Cloud library to visualise the most common words contained in spam messages.</p>\n",
    "\n",
    "[Example of a Word Cloud Image](https://drive.google.com/open?id=1lVRGHtMB1AMJf-JSi7MmcHbZB_BvBhGC)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2mmdY8jWbkgS"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a string containing all words in spam messages\n",
    "spam_words = ' '.join(train_data[train_data['v1'] == 'spam']['v2'])\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='black').generate(spam_words)\n",
    "\n",
    "# Plot word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title('Word Cloud for Spam Messages')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bVpFmJ5kX_HG"
   },
   "source": [
    "<h3>6.  Calculate $P\\left(E\\middle| S\\right)$ and $P\\left(E|\\lnot S\\right)$</h3>\n",
    "<p>Next create a new DataFrame named word_prob that gives the probability of each word being found in a spam and ham message.</p>\n",
    "<p>To calculate the probability of a word being spam you divide the number of times the word was found in spam by the total number of spam messages, likewise to calculate the probability of each word being found in a ham message you divide the number of times the word was found in a ham message by the total number of ham messages.</p>\n",
    "<p>If a word was not found in ham or spam it will cause problems later because the probability calculated will be zero. Therefore, use a pseudocount k and estimate the probability of seeing the word. This is known as smoothing and results in the following formula when k = 0.5, for example.</p>\n",
    "<p>$P\\left(E\\middle| S\\right)$ = (number of spams containing the word + k) / (total number of spam messages + 2 * k).</p>\n",
    "<p>Likewise, for $P\\left(E|\\lnot S\\right)$.</p>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Word</th>\n",
    "      <th>P(E|S)</th>\n",
    "      <th>P(E|¬¨S)</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>go</td>\n",
    "      <td>0.053322</td>\n",
    "      <td>0.050055</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>until</td>\n",
    "      <td>0.011364</td>\n",
    "      <td>0.004275</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>jurong</td>\n",
    "      <td>0.002622</td>\n",
    "      <td>0.000138</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>point</td>\n",
    "      <td>0.002622</td>\n",
    "      <td>0.002344</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>crazy</td>\n",
    "      <td>0.011364</td>\n",
    "      <td>0.002344</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2kaXmftX_HH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Pseudocount k\n",
    "k = 0.5\n",
    "\n",
    "# Total number of spam messages\n",
    "total_spam_messages = len(train_data[train_data['v1'] == 'spam'])\n",
    "\n",
    "# Total number of ham messages\n",
    "total_ham_messages = len(train_data[train_data['v1'] == 'ham'])\n",
    "\n",
    "# Initialize an empty dictionary to store word probabilities\n",
    "word_prob = {}\n",
    "\n",
    "# Iterate through each word in word_freq_df\n",
    "for word in word_freq_df.index:\n",
    "    # Calculate P(E|S)\n",
    "    spam_count = word_freq_df.loc[word, 'spam_count']\n",
    "    prob_word_given_spam = (spam_count + k) / (total_spam_messages + 2 * k)\n",
    "    \n",
    "    # Calculate P(E|¬¨S)\n",
    "    ham_count = word_freq_df.loc[word, 'ham_count']\n",
    "    prob_word_given_ham = (ham_count + k) / (total_ham_messages + 2 * k)\n",
    "    \n",
    "    # Store probabilities in the word_prob dictionary\n",
    "    word_prob[word] = {'prob_spam': prob_word_given_spam, 'prob_ham': prob_word_given_ham}\n",
    "\n",
    "# Create a DataFrame from the word_prob dictionary\n",
    "word_prob_df = pd.DataFrame(word_prob).transpose()\n",
    "\n",
    "# Output the first few rows of the word_prob DataFrame\n",
    "print(word_prob_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iagF4brWA5QO"
   },
   "source": [
    "<h3>7. Checking the 'spamliness' of a single word</h3>\n",
    "<p>Now that we have trained the model, we will test the model.  Before we use the test_data, first let‚Äôs check how the model calculates the spamliness of a single word.  This is where we use the Bayes Theorem formula.  We have already calculated $P\\left(E\\middle| S\\right)$ and $P\\left(E|\\lnot S\\right)$, so we can just extract these values from the word_prob DataFrame.</p>\n",
    "<p>We need to decide on the prior values $P\\left(S\\right)$ and $P\\left(\\lnot S\\right)$, this is where you can experiment and tweak the model, in this example the prior value for spam was set to $0.4$ and the prior value for not spam or ham was set to $0.6$.</p>\n",
    "<h3>\n",
    "$P\\left(S\\middle|\\ E\\right)=\\frac{P\\left(E\\middle|\\ S\\right)P\\left(S\\right)}{P\\left(E\\middle|\\ S\\right)P\\left(S\\right)+P\\left(E|\\lnot S\\right)P\\left(\\lnot S\\right)}$\n",
    "</h3>\n",
    "<pre>\n",
    "Output\n",
    "Word = ['free']\n",
    "P(E|S) = [0.29108392]\n",
    "P(E|¬¨S) = [0.01365141]\n",
    "P(S|E) = [0.93427577]\n",
    "P(¬¨S|E) = [0.06572423]\n",
    "</pre>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NS-KBvz4-YhJ"
   },
   "outputs": [],
   "source": [
    "# Prior probabilities\n",
    "prior_spam = 0.4\n",
    "prior_ham = 0.6\n",
    "\n",
    "# Function to calculate spamliness of a single word\n",
    "def calculate_spamliness(word):\n",
    "    # Retrieve P(E|S) and P(E|¬¨S) from word_prob_df\n",
    "    prob_word_given_spam = word_prob_df.loc[word, 'prob_spam']\n",
    "    prob_word_given_ham = word_prob_df.loc[word, 'prob_ham']\n",
    "    \n",
    "    # Calculate P(S|E) using Bayes' theorem formula\n",
    "    prob_spam_given_word = (prob_word_given_spam * prior_spam) / \\\n",
    "                           ((prob_word_given_spam * prior_spam) + (prob_word_given_ham * (1 - prior_spam)))\n",
    "    \n",
    "    return prob_spam_given_word\n",
    "\n",
    "# Example word to check\n",
    "word_to_check = 'offer'\n",
    "\n",
    "# Calculate spamliness of the word\n",
    "spamliness = calculate_spamliness(word_to_check)\n",
    "\n",
    "# Output the spamliness of the word\n",
    "print(f\"The spamliness of the word '{word_to_check}' is: {spamliness}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NiU4nDFrNW-7"
   },
   "source": [
    "<h3>8. Checking the 'spamliness' of several words</h3>\n",
    "<p>To check the spamliness of several words contained in a message we multiply the probabilities.  The model assumes the words appear as independent events hence the na√Øve Bayes.  In reality of course, words are not independent events, but the model still performs well.  So we use the assumption that the words appear independently, and hence we multiply probabilities, so\n",
    "$P(S\\,|\\, x_1,\\dots,x_n)\\approx \\frac{P(S)\\underset{i=1}{\\overset{n}{\\prod}}P(x_i | S)}{P(S)\\underset{i=1}{\\overset{n}{\\prod}}P(x_i | S)+P(\\neg S)\\underset{i=1}{\\overset{n}{\\prod}}P(x_i | \\neg S)}$\n",
    "\n",
    "Calculate the probability for each word in a message being spam, you might want to store the calculations in a list named prob_spam.  Likewise create a list for each word not being spam.\n",
    "Then multiply the probabilities and compare the results.  If the result of multiplying the probabilities for spam is greater than the result of multiplying the probabilities for not spam, then you assume the message as spam.\n",
    "</p>\n",
    "<p>If you have a word in your message that is not in the word_prob DataFrame then you can't get the probability.  Skip any words in the message that are not in the word_prob DataFrame.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PGK027jUNlU7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_message_spamliness(message):\n",
    "    words = message.split()\n",
    "    \n",
    "    prob_spam = []\n",
    "    prob_not_spam = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in word_prob_df.index:\n",
    "            # Retrieve P(E|S) and P(E|¬¨S) from word_prob_df\n",
    "            prob_word_given_spam = word_prob_df.loc[word, 'prob_spam']\n",
    "            prob_word_given_ham = word_prob_df.loc[word, 'prob_ham']\n",
    "            \n",
    "            prob_spam.append(prob_word_given_spam)\n",
    "            prob_not_spam.append(prob_word_given_ham)\n",
    "    \n",
    "    # Calculate the product of probabilities for spam and not spam\n",
    "    prod_prob_spam = prior_spam * np.prod(prob_spam)\n",
    "    prod_prob_not_spam = prior_ham * np.prod(prob_not_spam)\n",
    "    \n",
    "    if prod_prob_spam > prod_prob_not_spam:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'ham'\n",
    "\n",
    "# Example message to check\n",
    "message_to_check = \"Get your free offer now!\"\n",
    "\n",
    "# Calculate the spamliness of the message\n",
    "classification = calculate_message_spamliness(message_to_check)\n",
    "\n",
    "# Output the classification of the message\n",
    "print(f\"The message '{message_to_check}' is classified as: {classification}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oE6CsYaev-w_"
   },
   "source": [
    "<h3>9. Avoiding floating point underflow</h3>\n",
    "<p>Our aim is to compare two probabilities $P(S|x_1,\\dots,x_n)$ with $P(\\neg S|x_1,\\dots,x_n),$ according to our model introduced in Section 8, both probabilities share a common denominator which does not affect comparison. Hence we will calculate numerators only, which are proportional to $P(S|x_1,\\dots,x_n)$ and $P(\\neg S|x_1,\\dots,x_n).$\n",
    "</p>\n",
    "\n",
    "<p>Multiplying a set of small probabilities could result in a floating-point error.  This is where the product becomes too small to be represented correctly.  To avoid this we can take the logarithm of the probabilities and add them.  \n",
    "\n",
    "To avoid multiplication of small numbers, we use the following property of $\\log(x):$</p>\n",
    "$$\n",
    "\\log(a\\cdot b)=\\log(a)+\\log(b)\n",
    "$$\n",
    "<p>i.e. the log of the product is equal to the sum of logs (so instead of multiplying small numbers we will add them):</p>\n",
    "$$\n",
    "P(S|x_1,x_2,\\dots,x_n)\\propto P(S)\\cdot P(x_1|S)\\cdot \\dots \\cdot P(x_n|S)$$\n",
    "<p>becomes</p>\n",
    "$$\\log(P(S|x_1,x_2,\\dots,x_n))\\propto \\log\\left(P(S)\\cdot P(x_1|S)\\cdot \\dots  P(x_n|S)\\right)=$$ $$\n",
    "\\log(P(S))+\\log(P(x_1|S))+\\dots+\\log(P(x_n|S))\n",
    "$$\n",
    "<p>So, to check spam or ham we just compare:</p>\n",
    "$$\n",
    "\\log(P(S))+\\log(P(x_1|S))+\\dots+\\log(P(x_n|S))\n",
    "$$\n",
    "<p>and </p>\n",
    "$$\n",
    "\\log(P(\\neg S))+\\log(P(x_1|\\neg S))+\\dots+\\log(P(x_n|\\neg S))\n",
    "$$\n",
    "\n",
    "\n",
    "Change the equation so that logs are used.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for comparing, we evaluate:\n",
    "\n",
    "log(P(S)) + log(P(x1|S)) + ... + log(P(xn|S))\n",
    "\n",
    "and\n",
    "\n",
    "log(P(¬¨S)) + log(P(x1|¬¨S)) + ... + log(P(xn|¬¨S))\n",
    "\n",
    "These equations represent the comparison of probabilities using logarithms, where log denotes the natural logarithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZtiGi5X37b0G"
   },
   "source": [
    "<h3>10. Testing the Model</h3>\n",
    "<p>Now that we have tested the model using simple messages.  Let‚Äôs test the model using the messages from the test_set.  You should implement counters that displays how your model has performed and calculate the accuracy of the model.</p>\n",
    "<pre>\n",
    "match_spam 173\n",
    "match_ham 843\n",
    "thought_ham_is_spam 3\n",
    "thought_spam_is_ham 357\n",
    "Accuracy: 0.7383720930232558\n",
    "</pre>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOreoe6F7eri"
   },
   "outputs": [],
   "source": [
    "match_spam = 0\n",
    "match_ham = 0\n",
    "thought_ham_is_spam = 0\n",
    "thought_spam_is_ham = 0\n",
    "\n",
    "for index, row in test_data.iterrows():\n",
    "    message = row['v2']\n",
    "    label = row['v1']\n",
    "    \n",
    "    classification = calculate_message_spamliness(message)\n",
    "    \n",
    "    if classification == 'spam' and label == 'spam':\n",
    "        match_spam += 1\n",
    "    elif classification == 'ham' and label == 'ham':\n",
    "        match_ham += 1\n",
    "    elif classification == 'ham' and label == 'spam':\n",
    "        thought_ham_is_spam += 1\n",
    "    elif classification == 'spam' and label == 'ham':\n",
    "        thought_spam_is_ham += 1\n",
    "\n",
    "total_messages = len(test_data)\n",
    "accuracy = (match_spam + match_ham) / total_messages\n",
    "\n",
    "print(\"match_spam:\", match_spam)\n",
    "print(\"match_ham:\", match_ham)\n",
    "print(\"thought_ham_is_spam:\", thought_ham_is_spam)\n",
    "print(\"thought_spam_is_ham:\", thought_spam_is_ham)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fwGqqNBzDh3y"
   },
   "source": [
    "<h3>11. Improvements</h3>\n",
    "<p>Utilise the validation set to assess the performance of various word sets in classifying spam and non-spam (ham) emails. Compare the effectiveness of different sets of words to determine their impact on classification accuracy.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KM5hnY3WDA2f"
   },
   "source": [
    "<h3></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_words = set(word_freq_df.index)\n",
    "\n",
    "top_words = set(word_freq_df.sort_values(by='spam_count', ascending=False).head(1000).index)\n",
    "\n",
    "def calculate_message_spamliness_with_word_set(message, word_set):\n",
    "    prob_spam = []\n",
    "    prob_not_spam = []\n",
    "    \n",
    "    for word in message.split():\n",
    "        if word in word_set:\n",
    "            # Retrieve P(E|S) and P(E|¬¨S) from word_prob_df\n",
    "            prob_word_given_spam = word_prob_df.loc[word, 'prob_spam']\n",
    "            prob_word_given_ham = word_prob_df.loc[word, 'prob_ham']\n",
    "            \n",
    "            prob_spam.append(prob_word_given_spam)\n",
    "            prob_not_spam.append(prob_word_given_ham)\n",
    "    \n",
    "    prod_prob_spam = prior_spam + np.sum(np.log(prob_spam))\n",
    "    prod_prob_not_spam = prior_ham + np.sum(np.log(prob_not_spam))\n",
    "    \n",
    "    if prod_prob_spam > prod_prob_not_spam:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'ham'\n",
    "\n",
    "def evaluate_word_set(word_set):\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    for index, row in validation_data.iterrows():\n",
    "        message = row['v2']\n",
    "        label = row['v1']\n",
    "        \n",
    "        classification = calculate_message_spamliness_with_word_set(message, word_set)\n",
    "        \n",
    "        if classification == label:\n",
    "            correct_count += 1\n",
    "        total_count += 1\n",
    "    \n",
    "    accuracy = correct_count / total_count\n",
    "    return accuracy\n",
    "\n",
    "all_words_accuracy = evaluate_word_set(all_words)\n",
    "top_words_accuracy = evaluate_word_set(top_words)\n",
    "\n",
    "print(\"Accuracy with all words:\", all_words_accuracy)\n",
    "print(\"Accuracy with top words:\", top_words_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab05_spam_filter_student.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
